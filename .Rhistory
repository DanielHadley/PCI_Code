# Geo
# Creates a full geocodable address based on the location
Permits$FullAddress <- paste(Permits$ST., Permits$STREET, sep=" ")
# Assessors Data
# I usually make a "1" column to make tabulations easier
assessors$Tab <- 1
# aggregate works for a couple of variables.
# "Cast" from reshape2 works when you have more than two variables:
# http://marcoghislanzoni.com/blog/2013/10/11/pivot-tables-in-r-with-melt-and-cast/
library(reshape2)
sapply(assessors[1,],class) #look at these again to see which columns to include
names(assessors) #look at the names
data.m <- melt(assessors, id=c(7, 8), measure=c(9)) # id = non-numeric; measure = numeric
data.c <- dcast(data.m, Location ~ Fiscal.Year + variable, sum)
# Bind the two together
my.df <- merge(data.c, Permits, by.x="Location", by.y="FullAddress" , all=FALSE)
# write.csv(my.df, file = "PermitsVAssessors.csv")
my.df$'FY13.to.FY14' <- my.df$'2014_Total.Appraised.Bldg.Value' - my.df$'2013_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY13.to.FY14' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY13.to.FY14'), log(my.df$CostNumeric))
hist(log(my.df$delta))
View(data.c)
setwd("K:/Somerstat/Common/Data/2014 Permits v Assessing")
Permits <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/BuildingPermitsNine.csv")
assessors <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/mult_FY_value.csv")
Permits$FeeNumeric <- as.numeric(as.character(Permits$FEE))
Permits$CostNumeric <- as.numeric(as.character(Permits$COST))
Permits$Tab <- 1
Permits$Date <- as.Date(Permits$ISSUED,"%m/%d/%Y")
Permits$Month <- format(Permits$Date, format='%m')
Permits$Day <- format(Permits$Date, format='%d')
Permits$Year <- format(Permits$Date, format='%Y')
Permits$FullAddress <- paste(Permits$ST., Permits$STREET, sep=" ")
assessors$Tab <- 1
library(reshape2)
sapply(assessors[1,],class) #look at these again to see which columns to include
names(assessors) #look at the names
data.m <- melt(assessors, id=c(7, 8), measure=c(9)) # id = non-numeric; measure = numeric
data.c <- dcast(data.m, Location ~ Fiscal.Year + variable, sum)
my.df <- merge(data.c, Permits, by.x="Location", by.y="FullAddress" , all=FALSE)
View(my.df)
my.df$'FY10.to.FY11' <- my.df$'2011_Total.Appraised.Bldg.Value' - my.df$'2010_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY10.to.FY11' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY13.to.FY14'), log(my.df$CostNumeric))
plot(log(my.df$'FY10.to.FY11'), log(my.df$CostNumeric))
hist(log(my.df$delta))
close <- my.df [ which(my.df$delta < 1000 & my.df$delta > 0), ]
View(close)
my.df$'FY10.to.FY11' <- my.df$'2011_Total.Appraised.Bldg.Value' - my.df$'2010_Total.Appraised.Bldg.Value'
View(my.df)
# Cleans and analyzes data from the Building Data
# There is analysis of the building data below
setwd("K:/Somerstat/Common/Data/2014 Permits v Assessing")
Permits <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/BuildingPermitsNine.csv")
assessors <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/mult_FY_value.csv")
View(Permits)
# Cleans and analyzes data from the Building Data
# There is analysis of the building data below
setwd("K:/Somerstat/Common/Data/2014 Permits v Assessing")
Permits <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/BuildingPermitsNine.csv")
assessors <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/mult_FY_value.csv")
Permits$FeeNumeric <- as.numeric(as.character(Permits$FEE))
Permits$CostNumeric <- as.numeric(as.character(Permits$COST))
Permits$Tab <- 1
View(Permits)
## Date
Permits$Date <- as.Date(Permits$ISSUED,"%m/%d/%Y")
Permits$Month <- format(Permits$Date, format='%m')
Permits$Day <- format(Permits$Date, format='%d')
Permits$Year <- format(Permits$Date, format='%Y')
# Geo
# Creates a full geocodable address based on the location
Permits$FullAddress <- paste(Permits$ST., Permits$STREET, sep=" ")
# Assessors Data
# I usually make a "1" column to make tabulations easier
assessors$Tab <- 1
# aggregate works for a couple of variables.
# "Cast" from reshape2 works when you have more than two variables:
# http://marcoghislanzoni.com/blog/2013/10/11/pivot-tables-in-r-with-melt-and-cast/
library(reshape2)
sapply(assessors[1,],class) #look at these again to see which columns to include
names(assessors) #look at the names
data.m <- melt(assessors, id=c(7, 8), measure=c(9)) # id = non-numeric; measure = numeric
data.c <- dcast(data.m, Location ~ Fiscal.Year + variable, sum)
# Bind the two together
my.df <- merge(data.c, Permits, by.x="Location", by.y="FullAddress" , all=FALSE)
my.df$'FY10.to.FY11' <- my.df$'2011_Total.Appraised.Bldg.Value' - my.df$'2010_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY10.to.FY11' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY10.to.FY11'), log(my.df$CostNumeric))
close <- my.df [ which(my.df$delta < 1000 & my.df$delta > 0), ]
summary(my.df$delta)
my.df$'FY09.to.FY10' <- my.df$'2010_Total.Appraised.Bldg.Value' - my.df$'2009_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY09.to.FY10' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY09.to.FY10'), log(my.df$CostNumeric))
hist(log(my.df$delta))
hist(my.df$delta)
summary(my.df$delta)
close <- my.df [ which(my.df$delta < 1000 & my.df$delta > 0), ]
# Cleans and analyzes data from the Building Data
# There is analysis of the building data below
setwd("K:/Somerstat/Common/Data/2014 Permits v Assessing")
Permits <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/BuildingPermitsNine.csv")
assessors <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/mult_FY_value.csv")
#### Cleans Data ####
Permits$FeeNumeric <- as.numeric(as.character(Permits$FEE))
Permits$CostNumeric <- as.numeric(as.character(Permits$COST))
Permits$Tab <- 1
## Date
Permits$Date <- as.Date(Permits$ISSUED,"%m/%d/%Y")
Permits$Month <- format(Permits$Date, format='%m')
Permits$Day <- format(Permits$Date, format='%d')
Permits$Year <- format(Permits$Date, format='%Y')
# Geo
# Creates a full geocodable address based on the location
Permits$FullAddress <- paste(Permits$ST., Permits$STREET, sep=" ")
# Assessors Data
# I usually make a "1" column to make tabulations easier
assessors$Tab <- 1
# aggregate works for a couple of variables.
# "Cast" from reshape2 works when you have more than two variables:
# http://marcoghislanzoni.com/blog/2013/10/11/pivot-tables-in-r-with-melt-and-cast/
library(reshape2)
sapply(assessors[1,],class) #look at these again to see which columns to include
names(assessors) #look at the names
data.m <- melt(assessors, id=c(7, 8), measure=c(9)) # id = non-numeric; measure = numeric
data.c <- dcast(data.m, Location ~ Fiscal.Year + variable, sum)
my.df <- merge(data.c, Permits, by.x="Location", by.y="FullAddress" , all=FALSE)
my.df$'FY09.to.FY10' <- my.df$'2010_Total.Appraised.Bldg.Value' - my.df$'2009_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY09.to.FY10' - my.df$CostNumeric
hist(my.df$delta)
View(my.df)
pos <- my.df [ which(my.df$FY09.to.FY10 > 0), ]
View(pos)
hist(pos$delta)
plot(log(pos$'FY09.to.FY10'), log(pos$CostNumeric))
hist(log(pos$delta))
summary(pos$delta)
pos <- post [ which(pos$delta < 50000), ]
pos <- pos [ which(pos$delta < 50000), ]
summary(pos$delta)
pos <- pos [ which(pos$delta < 50000 & pos$delta > 0), ]
summary(pos$delta)
hist(pos$delta)
my.df$'FY09.to.FY11' <- my.df$'2011_Total.Appraised.Bldg.Value' - my.df$'2009_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY09.to.FY11' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY09.to.FY10'), log(my.df$CostNumeric))
hist(log(my.df$delta))
# Just positive change to weed out the ones in the wrong FY
pos <- my.df [ which(my.df$FY09.to.FY11 > 0), ]
hist(pos$delta)
plot(log(pos$'FY09.to.FY11'), log(pos$CostNumeric))
hist(log(pos$delta))
plot(log(pos$'FY09.to.FY11'), log(pos$CostNumeric))
summary(pos$delta)
View(pos)
hist(pos$delta)
pos <- pos [ which(pos$delta < 50000 & pos$delta > 0), ]
my.df$'FY09.to.FY11' <- my.df$'2011_Total.Appraised.Bldg.Value' - my.df$'2009_Total.Appraised.Bldg.Value'
my.df$delta <- my.df$'FY09.to.FY11' - my.df$CostNumeric
hist(my.df$delta)
plot(log(my.df$'FY09.to.FY10'), log(my.df$CostNumeric))
hist(log(my.df$delta))
# Just positive change to weed out the ones in the wrong FY
pos <- my.df [ which(my.df$FY09.to.FY11 > 0), ]
hist(pos$delta)
plot(log(pos$'FY09.to.FY11'), log(pos$CostNumeric))
hist(log(pos$delta))
summary(pos$delta)
pos2 <- pos [ which(pos$delta < 500000 & pos$delta > 0), ]
View(pos2)
View(pos2)
pos2 <- pos [ which(pos$delta < 900000 & pos$delta > 0), ]
hist(pos2$delta)
pos2 <- pos [ which(pos$delta < 30000 & pos$delta > 0), ]
hist(pos2$delta)
pos2 <- pos [ which(pos$delta < 300000 & pos$delta > 0), ]
pos2 <- pos [ which(pos$delta < 90000 & pos$delta > 0), ]
hist(pos2$delta)
plot(log(pos$'FY09.to.FY11'), log(pos$CostNumeric))
fit <- lm(CostNumeric ~ 'FY09.to.FY11', data=pos)
summary(fit) # show results
fit <- lm('CostNumeric' ~ 'FY09.to.FY11', data=pos)
fit <- lm(CostNumeric ~ FY09.to.FY11, data=pos)
summary(fit) # show results
# Cleans and analyzes data from the Building Data
# There is analysis of the building data below
setwd("K:/Somerstat/Common/Data/2014 Permits v Assessing")
Permits <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/BuildingPermits.csv")
my.df <- read.csv("K:/Somerstat/Common/Data/2014 Permits v Assessing/mult_FY_value.csv")
#### Cleans Data ####
Permits$FeeNumeric <- as.numeric(as.character(Permits$FEE))
Permits$CostNumeric <- as.numeric(as.character(Permits$COST))
Permits$Tab <- 1
## Date
Permits$Date <- as.Date(Permits$ISSUED,"%m/%d/%Y")
Permits$Month <- format(Permits$Date, format='%m')
Permits$Day <- format(Permits$Date, format='%d')
Permits$Year <- format(Permits$Date, format='%Y')
# Geo
# Creates a full geocodable address based on the location
Permits$FullAddress <- paste(Permits$ST., Permits$STREET, sep=" ")
# Assessors Data
# I usually make a "1" column to make tabulations easier
my.df$Tab <- 1
View(Permits)
summary(Permits$Fee)
setwd("K:/Somerstat/Common/Data/2014 StreetStat/PCI_Code")
d <- read.csv("PCI.csv")
pwd
pwd()
getwd
getwd()
setwd (/Users/dphnrome/Documents/Git/PCI_Code)
setwd (Users/dphnrome/Documents/Git/PCI_Code)
PCI <- read.csv("~/Documents/Git/PCI_Code/PCI.csv", header=F)
View(PCI)
setwd (~/Documents/Git/PCI_Code)
setwd("~/Documents/Git/PCI_Code")
setwd ("~/Documents/Git/PCI_Code")
d <- read.csv("PCI.csv")
# Create new variables
d$sq.ft <- d$PavementWi * d$Length # Sq. Feet
d$sq.yd <- d$sq.ft * 0.111111 # Sq. Yards
d$total.pci <- d$sq.yd * d$OCI # Sum PCI
d$ideal.pci <- d$sq.yd * 95 # Ideal sum PCI
d$delta.pci <- d$ideal.pci - d$total.pci #Difference
d$delta.over.cost <- d$delta.pci / d$ExtendedCo #One cost measure
aggregate(delta.over.cost ~ PlanActivi, d, mean ) # Crack Seal is crazy cost efficient
aggregate(delta.over.cost ~ PlanActivi, d, max ) # Crack Seal is crazy cost efficient
d$cost.per.sq.yd <- d$ExtendedCo / d$sq.yd # Cost per sq yard
# Here I model the age as a function of PCI based on the references below:
# It is an average of residential and arterial, but we can divide those later
# http://onlinepubs.trb.org/onlinepubs/conferences/2012/assetmgmt/presentations/Data-A-Ramirez-Flores-Chang-Albitres.pdf
# https://repository.tamu.edu/bitstream/handle/1969.1/ETD-TAMU-2009-05-317/DESHMUKH-THESIS.pdf?sequence=2
# http://www.mylongview.com/modules/showdocument.aspx?documentid=631
# PCI = 100 - (106/((ln(79/AGE))^(1/.48)))
d$est.years <- 79*(2.71828^(-9.37879/(100-d$OCI)^0.48))
plot(d$est.years, d$OCI)
d$last.paved <- 2012 - d$est.years
hist(d$last.paved) # I'm skeptical there are streets we have not paved since the 80s
# test
plot(log(d$ExtendedCo), d$OCI)
plot(d$delta.over.cost, d$OCI) # checking cost-effectiveness
# This final plot is where we see that high OCI maintenance is the most cost-effective
# First model cost as a f(PCI) using logical tests:
# this averages differences between collectors and arterials
# To see where the bands begin and end
aggregate(OCI ~ PlanActivi, d, mean )
aggregate(OCI ~ PlanActivi, d, min )
aggregate(OCI ~ PlanActivi, d, max )
d$cost.per.sq.yd.conditional <- ifelse((d$OCI >= 68) & (d$OCI < 88), 1.8,
ifelse((d$OCI >= 47) & (d$OCI < 68), 18.50,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "RT - Residential Local"), 76.80,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "RE - Residential Dead End"), 76.80,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "CO - Collector" ), 91.10,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "AR - Arterial"), 91.10,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "RT - Residential Local"), 139.80,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "RE - Residential Dead End"), 139.80,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "CO - Collector"), 147.70,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "AR - Arterial"), 162.10,
ifelse(d$OCI >= 88, 0, 360)))))))))))
# Test
plot(d$cost.per.sq.yd.conditional*d$sq.yd, d$cost.per.sq.yd*d$sq.yd)
plot(log(d$cost.per.sq.yd.conditional*d$sq.yd), log(d$cost.per.sq.yd*d$sq.yd))
# d$dif <- d$cost.per.sq.yd*d$sq.yd - d$cost.per.sq.yd.conditional*d$sq.yd
# hugeDif <- d[ which(d$dif>650000 | d$dif < -115000), ]
# # Now model the cost degredation as a f(PCI) using a smooth curve
# # This smooths out all of the "cliffs" from the difference maintenance bands,
# # but it approximates how quickly costs escalate as a f of PCI and allows for linear optimization
#
# ####  Use the empirical distribution of PCI ####
# # http://davetang.org/muse/2013/05/09/on-curve-fitting/
# y = d$cost.per.sq.yd
# x = d$OCI
# plot(x,y)
#
# #fit first degree polynomial equation:
# fit  <- lm(y~x)
# #second degree
# fit2 <- lm(y~poly(x,2,raw=TRUE))
# #third degree
# fit3 <- lm(y~poly(x,3,raw=TRUE))
# #fourth degree
# fit4 <- lm(y~poly(x,4,raw=TRUE))
# #generate range of 50 numbers starting from 30 and ending at 160
# xx <- seq(0,160, length=50)
# plot(x,y,pch=19,ylim=c(0,150))
# lines(xx, predict(fit, data.frame(x=xx)), col="red")
# lines(xx, predict(fit2, data.frame(x=xx)), col="green")
# lines(xx, predict(fit3, data.frame(x=xx)), col="blue")
# lines(xx, predict(fit4, data.frame(x=xx)), col="purple") ## This looks like the best fit
#
# # Now model it based on the coefficients from fit4
# #y=e + dx + cx^2 + bx^3 + ax^4
# coef(fit4)
#
# d$cost.model <- 1.351524e+02 + (1.845730e+00 * d$OCI) +
#   (-1.630135e-01 * d$OCI^2) +
#   (2.195987e-03 * d$OCI^3) +
#   (-8.857414e-06 * d$OCI^4)
#
#
# fit <- lm(d$cost.model,d$cost.per.sq.yd)
# summary(fit) # show results
#
# # Visualize
# plot(d$cost.model*d$sq.yd, d$cost.per.sq.yd*d$sq.yd)
# plot(log(d$cost.model*d$sq.yd), log(d$cost.per.sq.yd*d$sq.yd))
#
# # Damn, nothing seems to model the cliffs correctly
###  Model the Pavement decisions over 20 years ###
dm <- d # dm = data model
dm <- subset(dm, select = c(FID, Functional, STREETNAME, OCI, sq.yd)) #OR
PCIF <- function(AGE){
dm$PCI <- 100 - (106/((ln(79/AGE))^(1/.48)))
return(dm$PCI)
}
PCIF(d$est.years)
# F(Age) = PCI
PCIF <- function(AGE){
dm$PCI <- 100 - (106/((log(79/AGE))^(1/.48)))
return(dm$PCI)
}
PCIF(d$est.years)
d$OCI
# F(Age) = PCI
PCIF <- function(AGE){
PCI <- 100 - (106/((log(79/AGE))^(1/.48)))
return(PCI)
}
PCIF(d$est.years)
d <- read.csv("PCI.csv")
View(d)
d$sq.ft <- d$PavementWi * d$Length # Sq. Feet
d$sq.yd <- d$sq.ft * 0.111111 # Sq. Yards
d$total.pci <- d$sq.yd * d$OCI # Sum PCI
d$ideal.pci <- d$sq.yd * 95 # Ideal sum PCI
d$delta.pci <- d$ideal.pci - d$total.pci #Difference
d$delta.over.cost <- d$delta.pci / d$ExtendedCo #One cost measure
aggregate(delta.over.cost ~ PlanActivi, d, mean ) # Crack Seal is crazy cost efficient
aggregate(delta.over.cost ~ PlanActivi, d, max ) # Crack Seal is crazy cost efficient
d$cost.per.sq.yd <- d$ExtendedCo / d$sq.yd # Cost per sq yard
d$est.years <- 79*(2.71828^(-9.37879/(100-d$OCI)^0.48))
plot(d$est.years, d$OCI)
d$last.paved <- 2012 - d$est.years
hist(d$last.paved) # I'm skeptical there are streets we have not paved since the 80s
# test
plot(log(d$ExtendedCo), d$OCI)
plot(d$delta.over.cost, d$OCI) # checking cost-effectiveness
# This final plot is where we see that high OCI maintenance is the most cost-effective
# First model cost as a f(PCI) using logical tests:
# this averages differences between collectors and arterials
# To see where the bands begin and end
aggregate(OCI ~ PlanActivi, d, mean )
aggregate(OCI ~ PlanActivi, d, min )
aggregate(OCI ~ PlanActivi, d, max )
d$cost.per.sq.yd.conditional <- ifelse((d$OCI >= 68) & (d$OCI < 88), 1.8,
ifelse((d$OCI >= 47) & (d$OCI < 68), 18.50,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "RT - Residential Local"), 76.80,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "RE - Residential Dead End"), 76.80,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "CO - Collector" ), 91.10,
ifelse((d$OCI >= 25) & (d$OCI < 47) & (d$Functional == "AR - Arterial"), 91.10,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "RT - Residential Local"), 139.80,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "RE - Residential Dead End"), 139.80,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "CO - Collector"), 147.70,
ifelse((d$OCI >= 0) & (d$OCI < 25) & (d$Functional == "AR - Arterial"), 162.10,
ifelse(d$OCI >= 88, 0, 360)))))))))))
# Test
plot(d$cost.per.sq.yd.conditional*d$sq.yd, d$cost.per.sq.yd*d$sq.yd)
plot(log(d$cost.per.sq.yd.conditional*d$sq.yd), log(d$cost.per.sq.yd*d$sq.yd))
# d$dif <- d$cost.per.sq.yd*d$sq.yd - d$cost.per.sq.yd.conditional*d$sq.yd
# hugeDif <- d[ which(d$dif>650000 | d$dif < -115000), ]
# # Now model the cost degredation as a f(PCI) using a smooth curve
# # This smooths out all of the "cliffs" from the difference maintenance bands,
# # but it approximates how quickly costs escalate as a f of PCI and allows for linear optimization
#
# ####  Use the empirical distribution of PCI ####
# # http://davetang.org/muse/2013/05/09/on-curve-fitting/
# y = d$cost.per.sq.yd
# x = d$OCI
# plot(x,y)
#
# #fit first degree polynomial equation:
# fit  <- lm(y~x)
# #second degree
# fit2 <- lm(y~poly(x,2,raw=TRUE))
# #third degree
# fit3 <- lm(y~poly(x,3,raw=TRUE))
# #fourth degree
# fit4 <- lm(y~poly(x,4,raw=TRUE))
# #generate range of 50 numbers starting from 30 and ending at 160
# xx <- seq(0,160, length=50)
# plot(x,y,pch=19,ylim=c(0,150))
# lines(xx, predict(fit, data.frame(x=xx)), col="red")
# lines(xx, predict(fit2, data.frame(x=xx)), col="green")
# lines(xx, predict(fit3, data.frame(x=xx)), col="blue")
# lines(xx, predict(fit4, data.frame(x=xx)), col="purple") ## This looks like the best fit
#
# # Now model it based on the coefficients from fit4
# #y=e + dx + cx^2 + bx^3 + ax^4
# coef(fit4)
#
# d$cost.model <- 1.351524e+02 + (1.845730e+00 * d$OCI) +
#   (-1.630135e-01 * d$OCI^2) +
#   (2.195987e-03 * d$OCI^3) +
#   (-8.857414e-06 * d$OCI^4)
#
#
# fit <- lm(d$cost.model,d$cost.per.sq.yd)
# summary(fit) # show results
#
# # Visualize
# plot(d$cost.model*d$sq.yd, d$cost.per.sq.yd*d$sq.yd)
# plot(log(d$cost.model*d$sq.yd), log(d$cost.per.sq.yd*d$sq.yd))
#
# # Damn, nothing seems to model the cliffs correctly
###  Model the Pavement decisions over 20 years ###
dm <- d # dm = data model
dm <- subset(dm, select = c(FID, Functional, STREETNAME, OCI, sq.yd)) #OR
View(d)
# F(PCI) = Cost
CostF <- function(OCI, Functional, sq.yd){
Cost <- ifelse((OCI >= 68) & (OCI < 88), 1.8,
ifelse((OCI >= 47) & (OCI < 68), 18.50,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "RT - Residential Local"), 76.80,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "RE - Residential Dead End"), 76.80,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "CO - Collector" ), 91.10,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "AR - Arterial"), 91.10,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "RT - Residential Local"), 139.80,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "RE - Residential Dead End"), 139.80,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "CO - Collector"), 147.70,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "AR - Arterial"), 162.10,
ifelse(OCI >= 88, 0, 360)))))))))))
return(Cost*sq.yd)
}
CostF(d$OCI, d$Functional, d$sq.yd)
# f(year) = backlog
myfunction <- function(year){
d$pave <- sample(c(0,1), 573, replace = TRUE)
AgeA <- ifelse(d$pave == 1, 1, year + 1)
OCIA <- ifelse(d$pave = 1, 93, 0)
cost <- CostF(OCIA, d$Functional, d$sq.yd)
backlog <- sum(cost)
return(backlog)
}
# f(year) = backlog
myfunction <- function(year){
d$pave <- sample(c(0,1), 573, replace = TRUE)
AgeA <- ifelse(d$pave == 1, 1, year + 1)
OCIA <- ifelse(d$pave =+ 1, 93, 0)
cost <- CostF(OCIA, d$Functional, d$sq.yd)
backlog <- sum(cost)
return(backlog)
}
# First I create functions that describe the relationships analyzed previously
# f(Age) = PCI
PCIf <- function(AGE){
PCI <- 100 - (106/((log(79/AGE))^(1/.48)))
return(PCI)
}
# f(PCI) = Cost
Costf <- function(OCI, Functional, sq.yd){
Cost <- ifelse((OCI >= 68) & (OCI < 88), 1.8,
ifelse((OCI >= 47) & (OCI < 68), 18.50,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "RT - Residential Local"), 76.80,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "RE - Residential Dead End"), 76.80,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "CO - Collector" ), 91.10,
ifelse((OCI >= 25) & (OCI < 47) & (Functional == "AR - Arterial"), 91.10,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "RT - Residential Local"), 139.80,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "RE - Residential Dead End"), 139.80,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "CO - Collector"), 147.70,
ifelse((OCI >= 0) & (OCI < 25) & (Functional == "AR - Arterial"), 162.10,
ifelse(OCI >= 88, 0, 360)))))))))))
return(Cost*sq.yd)
}
# f(year) = backlog
myfunction <- function(year){
d$pave <- sample(c(0,1), 573, replace = TRUE)
d$AgeA <- ifelse(d$pave == 1, 1, year + 1)
d$OCIA <- ifelse(d$pave == 1, 93, 0)
cost <- CostF(OCIA, d$Functional, d$sq.yd)
backlog <- sum(cost)
return(backlog)
}
myfunction(d$est.years)
# f(year) = backlog
myfunction <- function(year){
d$pave <- sample(c(0,1), 573, replace = TRUE)
d$AgeA <- ifelse(d$pave == 1, 1, year + 1)
d$OCIA <- ifelse(d$pave == 1, 93, 0)
cost <- CostF(d$OCIA, d$Functional, d$sq.yd)
backlog <- sum(cost)
return(backlog)
}
myfunction(d$est.years)
# f(year) = backlog
myfunction <- function(year){
d$pave <- sample(c(0,1), 573, replace = TRUE)
d$AgeA <- ifelse(d$pave == 1, 1, year + 1)
d$OCIA <- ifelse(d$pave == 1, 93, 0)
cost <- CostF(d$OCIA, d$Functional, d$sq.yd)
backlog <- sum(cost)
return(backlog)
}
myfunction(d$est.years)
View(d)
sum(d$ExtendedCo)
